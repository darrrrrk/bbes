{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import warnings\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import dgl\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "df = pd.read_csv('training_5rep_fraclabel.csv')\n",
    "df.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    return adj\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def graphencoder(adj):\n",
    "    source = []\n",
    "    target = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        for j in range(adj.shape[1]):\n",
    "            if adj[i, j]:\n",
    "                source.append(i)\n",
    "                target.append(j)\n",
    "    source = torch.from_numpy(np.array(source))\n",
    "    target = torch.from_numpy(np.array(target)) \n",
    "\n",
    "    adj = dgl.add_self_loop(dgl.graph((source, target),  num_nodes = 21))\n",
    "   \n",
    "    return adj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromCSV(Dataset):\n",
    "    def __init__(self, data):\n",
    " \n",
    "        self.data = data.drop(\"label\", axis = 1)\n",
    "        self.labels = np.asarray(data[\"label\"])\n",
    "        \n",
    "        #self.transforms = transforms\n",
    "        self.train_X, self.test_X, self.train_y, self.test_y = train_test_split(self.data,\n",
    "                                                   self.labels,\n",
    "                                                   train_size = 0.8,\n",
    "                                                   random_state = 1)\n",
    "        b, s ,t, w= np.unique(self.train_y,return_counts=True,return_index=True,return_inverse=True)\n",
    "        \n",
    "\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        single_ec_label = self.train_y[index]\n",
    "        adj = self.train_X.values[index][0:441].reshape(21, 21)\n",
    "        features = self.train_X.values[index][441:1638].reshape(21, 57)\n",
    "        \n",
    "        \n",
    "        nor_adj = graphencoder(adj)\n",
    "        \n",
    "        #nor_adj = (adj + sp.eye(adj.shape[0]))\n",
    "        nor_features = normalize(features)\n",
    "        \n",
    "        #nor_adj = torch.from_numpy(nor_adj)\n",
    "        nor_features = torch.from_numpy(nor_features)\n",
    "        nor_adj.ndata['feats'] = nor_features\n",
    "    \n",
    "            \n",
    "        return (nor_adj, single_ec_label) \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.train_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetFromCSV(Dataset):\n",
    "    def __init__(self, data):\n",
    " \n",
    "        self.data = data.drop(\"label\", axis = 1)\n",
    "        self.labels = np.asarray(data[\"label\"])\n",
    "        \n",
    "        #self.transforms = transforms\n",
    "        self.train_X, self.test_X, self.train_y, self.test_y = train_test_split(self.data,\n",
    "                                                   self.labels,\n",
    "                                                   train_size = 0.8,\n",
    "                                                   random_state = 1)\n",
    "        b, s ,t, w= np.unique(self.train_y,return_counts=True,return_index=True,return_inverse=True)\n",
    "        \n",
    "\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        single_ec_label = self.test_y[index]\n",
    "        adj = self.test_X.values[index][0:441].reshape(21, 21)\n",
    "        features = self.test_X.values[index][441:1638].reshape(21, 57)\n",
    "    \n",
    "        nor_adj = graphencoder(adj)\n",
    "        \n",
    "        #nor_adj = (adj + sp.eye(adj.shape[0]))\n",
    "        nor_features = normalize(features)\n",
    "        \n",
    "        #nor_adj = torch.from_numpy(nor_adj)\n",
    "        nor_features = torch.from_numpy(nor_features)\n",
    "        nor_adj.ndata['feats'] = nor_features\n",
    "    \n",
    "            \n",
    "        return (nor_adj, single_ec_label) \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.test_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \n",
    "    graphs, labels = zip(*samples)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    \n",
    "    batched_labels = torch.tensor(labels)\n",
    "    \n",
    "    return batched_graph, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(\n",
    "    DatasetFromCSV(df),\n",
    "    batch_size=1,\n",
    "    collate_fn=collate,\n",
    "    drop_last=False,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DatasetFromCSV(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_t = DataLoader(\n",
    "    TestDatasetFromCSV(df),\n",
    "    batch_size=1,\n",
    "    collate_fn=collate,\n",
    "    drop_last=False,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TestDatasetFromCSV(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import edge_softmax, GATConv\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
    "\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l - 1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None))\n",
    "\n",
    "        #self.bn = nn.BatchNorm1d(num_hidden)\n",
    "        self.fc = nn.Linear(num_classes, 1)\n",
    "#         self.do = nn.Dropout(p = 0.5)\n",
    "#         self.fc2 = nn.Linear(16, 8)\n",
    "#         self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        # output projection\n",
    "        h = self.gat_layers[-1](g, h).mean(1)\n",
    "        #h = self.bn(h)\n",
    "        with g.local_scope():\n",
    "#             h = F.relu(self.do(self.fc(h)))\n",
    "#             h = F.relu(self.fc2(h))\n",
    "#             h = self.fc3(h)\n",
    "            #h = self.fc2(h)\n",
    "            g.ndata['h'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = dgl.mean_nodes(g, \"h\")\n",
    "            # print(hg.size())\n",
    "\n",
    "            return self.fc(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heads = ([3] * 3) + [1] \n",
    "model = GAT(num_layers = 3,\n",
    "          in_dim = 57,\n",
    "          num_hidden = 8,\n",
    "          num_classes = 6,\n",
    "          heads = heads,\n",
    "          activation = F.elu,\n",
    "          feat_drop = 0。1,\n",
    "          attn_drop = 0,\n",
    "          negative_slope = 0,\n",
    "          residual = False)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "running_loss = 0\n",
    "test_loss = 0\n",
    "total = 0\n",
    "total_t = 0\n",
    "epoch_num = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    epoch_num += 1\n",
    "    total = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        feats = batched_graph.ndata['feats']\n",
    "        opt.zero_grad()\n",
    "        model.train()\n",
    "        outputs = model(batched_graph, feats)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)  \n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        i += 1\n",
    "\n",
    "        if i % 15000 == 14999:\n",
    "            correct = 0\n",
    "            total_t = 0\n",
    "            with torch.no_grad():\n",
    "                j = 0\n",
    "                for batched_graph_t, labels_t in dataloader_t:\n",
    "\n",
    "                    feats_t = batched_graph_t.ndata['feats']\n",
    "                    model.eval()\n",
    "                    outputs_t = model(batched_graph_t, feats_t)\n",
    "\n",
    "                    #loss_t = nn.CrossEntropyLoss()(outputs_t, labels_t)\n",
    "                    loss_t = nn.MSELoss()(outputs_t, labels_t)\n",
    "                    test_loss += loss_t.item()\n",
    "\n",
    "                    #total_t += labels_t.size(0)  \n",
    "                    #_, predicted = torch.max(outputs_t.data, 1)\n",
    "                    #correct += (predicted == labels_t).sum().item()\n",
    "                    #print(predicted, labels_t)\n",
    "                    j += 1\n",
    "\n",
    "            print('epoch %d' % epoch_num, 'training_loss: %.4f' % (running_loss / 10000), 'test_loss: %.4f' %(test_loss /j)) #'Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total_t))           \n",
    "            running_loss = 0.0\n",
    "            test_loss = 0\n",
    "            total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "       \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "      \n",
    "       \n",
    "        }, \"gat_oct18_test.pt\")\n",
    "\n",
    "print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model]",
   "language": "python",
   "name": "conda-env-model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
